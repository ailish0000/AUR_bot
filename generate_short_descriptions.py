#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç short_description –∏ short_benefits –∏–∑ –ø–æ–ª–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
"""

import json
import re
from typing import List, Dict, Any

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    text = text.strip()
    return text

def extract_main_benefits(benefits: List[str], max_count: int = 3) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
    if not benefits:
        return []
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞)
    sorted_benefits = sorted(benefits, key=lambda x: (len(x), x))
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ max_count
    main_benefits = sorted_benefits[:max_count]
    
    # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏
    shortened_benefits = []
    for benefit in main_benefits:
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
        benefit = re.sub(r'^–ø–æ–º–æ—â—å –ø—Ä–∏\s+', '', benefit)
        benefit = re.sub(r'^–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏\s+', '', benefit)
        benefit = re.sub(r'^–æ–±–ª–∞–¥–∞–µ—Ç\s+', '', benefit)
        benefit = re.sub(r'^–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞\s+', '', benefit)
        
        # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–æ 50 —Å–∏–º–≤–æ–ª–æ–≤
        if len(benefit) > 50:
            benefit = benefit[:47] + "..."
        
        shortened_benefits.append(benefit)
    
    return shortened_benefits

def create_short_description(description: str, max_length: int = 150) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ"""
    if not description:
        return ""
    
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
    clean_desc = clean_text(description)
    
    # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ —É–∂–µ –∫–æ—Ä–æ—Ç–∫–æ–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if len(clean_desc) <= max_length:
        return clean_desc
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'[.!?]+', clean_desc)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if not sentences:
        return clean_desc[:max_length]
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    short_desc = sentences[0]
    
    # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, –æ–±—Ä–µ–∑–∞–µ–º
    if len(short_desc) > max_length:
        # –ò—â–µ–º –º–µ—Å—Ç–æ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ (–ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –ø—Ä–æ–±–µ–ª–∞)
        words = short_desc.split()
        current = ""
        for word in words:
            if len(current + " " + word) <= max_length - 3:
                current += (" " + word) if current else word
            else:
                break
        
        short_desc = current + "..."
    
    # –ï—Å–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–µ, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Ç–æ—Ä–æ–µ
    elif len(sentences) > 1 and len(short_desc) < max_length // 2:
        second_sentence = sentences[1]
        combined = short_desc + ". " + second_sentence
        
        if len(combined) <= max_length:
            short_desc = combined
        else:
            # –û–±—Ä–µ–∑–∞–µ–º –≤—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            remaining = max_length - len(short_desc) - 3  # ". " + "..."
            if remaining > 0:
                short_desc = short_desc + ". " + second_sentence[:remaining] + "..."
    
    return short_desc

def process_product(product: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç, –¥–æ–±–∞–≤–ª—è—è –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
    result = product.copy()
    
    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    if "description" in product and "short_description" not in product:
        short_desc = create_short_description(product["description"])
        result["short_description"] = short_desc
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è {product['product']}: {short_desc[:50]}...")
    
    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è
    if "benefits" in product and "short_benefits" not in product:
        short_benefits = extract_main_benefits(product["benefits"])
        result["short_benefits"] = short_benefits
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –∫—Ä–∞—Ç–∫–∏–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {product['product']}: {short_benefits}")
    
    return result

def generate_short_descriptions(input_file: str = "knowledge_base.json", 
                              output_file: str = "knowledge_base_updated.json"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"""
    
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            products = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    except json.JSONDecodeError:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è JSON —Ñ–∞–π–ª–∞ {input_file}")
        return
    
    print(f"üì¶ –ù–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–≤–∞—Ä
    updated_products = []
    updated_count = 0
    
    for product in products:
        original_keys = set(product.keys())
        updated_product = process_product(product)
        updated_keys = set(updated_product.keys())
        
        if updated_keys != original_keys:
            updated_count += 1
        
        updated_products.append(updated_product)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(updated_products, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} —Ç–æ–≤–∞—Ä–æ–≤")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        for product in updated_products:
            if "short_description" in product:
                desc_len = len(product["short_description"])
                full_len = len(product.get("description", ""))
                reduction = ((full_len - desc_len) / full_len * 100) if full_len > 0 else 0
                print(f"   {product['product']}: {desc_len}/{full_len} —Å–∏–º–≤–æ–ª–æ–≤ ({reduction:.1f}% —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

def backup_original_file(filename: str = "knowledge_base.json"):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    import shutil
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_name = f"{filename}.backup_{timestamp}"
    
    try:
        shutil.copy2(filename, backup_name)
        print(f"üíæ –°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_name}")
        return backup_name
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é: {e}")
        return None

if __name__ == "__main__":
    print("üöÄ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
    backup_file = backup_original_file()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
    generate_short_descriptions()
    
    print("\nüéâ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("\nüìù –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª knowledge_base_updated.json")
    print("2. –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç, –∑–∞–º–µ–Ω–∏—Ç–µ knowledge_base.json")
    print("3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç: python test_short_descriptions.py")






