# vector_db.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
import json
import os
import time
import hashlib
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
load_dotenv()

# –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant Cloud
client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY"),
    https=True
)

# –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (–µ—Å–ª–∏ –µ—â—ë –Ω–µ —Å–æ–∑–¥–∞–Ω–∞)
try:
    client.create_collection(
        collection_name="aurora_knowledge",
        vectors_config=VectorParams(size=384, distance=Distance.COSINE)
    )
    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'aurora_knowledge' —Å–æ–∑–¥–∞–Ω–∞")
except:
    print("‚ÑπÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

# –•—ç—à –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
def get_file_hash():
    return hashlib.md5(open("knowledge_base.json", "rb").read()).hexdigest()

last_hash = get_file_hash()

def load_knowledge():
    with open("knowledge_base.json", "r", encoding="utf-8") as f:
        return json.load(f)

def index_knowledge():
    global last_hash
    try:
        current_hash = get_file_hash()
        if current_hash == last_hash:
            return  # –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π

        print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ knowledge_base.json ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É...")
        knowledge = load_knowledge()
        points = []

        for item in knowledge:
            text = (
                f"–ü—Ä–æ–¥—É–∫—Ç: {item['product']}\n"
                f"–û–ø–∏—Å–∞–Ω–∏–µ: {item['description']}\n"
                f"–ü–æ–ª—å–∑–∞: {', '.join(item['benefits'])}\n"
                f"–°–æ—Å—Ç–∞–≤: {item['composition']}\n"
                f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {item['dosage']}\n"
                f"–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è: {item['contraindications']}"
            )
            vector = model.encode(text).tolist()
            point_id = hashlib.md5(item['id'].encode()).hexdigest()

            points.append(
                PointStruct(
                    id=point_id,
                    vector=vector,
                    payload=item
                )
            )

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ç–æ—á–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
        client.recreate_collection(
            collection_name="aurora_knowledge",
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )
        client.upload_points(collection_name="aurora_knowledge", points=points)
        last_hash = current_hash

        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(points)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ Qdrant: {e}")

# –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
def start_auto_update():
    def auto_update():
        while True:
            time.sleep(10)
            try:
                index_knowledge()
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {e}")
    import threading
    thread = threading.Thread(target=auto_update, daemon=True)
    thread.start()