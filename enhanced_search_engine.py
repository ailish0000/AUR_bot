#!/usr/bin/env python3
"""
–£—Å–∏–ª–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
"""

import re
import json
import hashlib
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
from difflib import SequenceMatcher

@dataclass
class SearchStrategy:
    """–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞"""
    name: str
    priority: int
    weight: float
    description: str

@dataclass
class EnhancedSearchResult:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    content: str
    product: str
    chunk_type: str
    relevance_score: float
    confidence: float
    strategies_used: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = ""

class SearchStrategyType(Enum):
    """–¢–∏–ø—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞"""
    EXACT_MATCH = "exact_match"
    SEMANTIC_SEARCH = "semantic_search" 
    FUZZY_MATCH = "fuzzy_match"
    SYNONYM_EXPANSION = "synonym_expansion"
    CATEGORICAL_SEARCH = "categorical_search"
    COMPOSITIONAL_SEARCH = "compositional_search"
    HEALTH_CONDITION_SEARCH = "health_condition_search"
    INGREDIENT_SEARCH = "ingredient_search"
    BRAND_SEARCH = "brand_search"
    FALLBACK_BROAD = "fallback_broad"

class EnhancedSearchEngine:
    """–£—Å–∏–ª–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
    
    def __init__(self, vector_db=None, nlp_processor=None):
        self.vector_db = vector_db
        self.nlp_processor = nlp_processor
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        self.search_strategies = {
            SearchStrategyType.EXACT_MATCH: SearchStrategy(
                "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ", 1, 1.0, "–ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑ –∏ –Ω–∞–∑–≤–∞–Ω–∏–π"
            ),
            SearchStrategyType.SEMANTIC_SEARCH: SearchStrategy(
                "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫", 2, 0.9, "–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É"
            ),
            SearchStrategyType.SYNONYM_EXPANSION: SearchStrategy(
                "–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏", 3, 0.8, "–ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω–æ–Ω–∏–º–æ–≤"
            ),
            SearchStrategyType.CATEGORICAL_SEARCH: SearchStrategy(
                "–ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", 4, 0.7, "–ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤"
            ),
            SearchStrategyType.HEALTH_CONDITION_SEARCH: SearchStrategy(
                "–ü–æ–∏—Å–∫ –ø–æ –∑–¥–æ—Ä–æ–≤—å—é", 5, 0.85, "–ü–æ–∏—Å–∫ –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º –∑–¥–æ—Ä–æ–≤—å—è"
            ),
            SearchStrategyType.INGREDIENT_SEARCH: SearchStrategy(
                "–ü–æ–∏—Å–∫ –ø–æ —Å–æ—Å—Ç–∞–≤—É", 6, 0.75, "–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º"
            ),
            SearchStrategyType.FUZZY_MATCH: SearchStrategy(
                "–ù–µ—á–µ—Ç–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ", 7, 0.6, "–ü–æ–∏—Å–∫ —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏ –∏ –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏"
            ),
            SearchStrategyType.BRAND_SEARCH: SearchStrategy(
                "–ü–æ–∏—Å–∫ –ø–æ –±—Ä–µ–Ω–¥–∞–º", 8, 0.65, "–ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤ –∏ –ª–∏–Ω–µ–µ–∫"
            ),
            SearchStrategyType.COMPOSITIONAL_SEARCH: SearchStrategy(
                "–ö–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫", 9, 0.55, "–ü–æ–∏—Å–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤"
            ),
            SearchStrategyType.FALLBACK_BROAD: SearchStrategy(
                "–®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫", 10, 0.3, "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫"
            )
        }
        
        # –ë–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.knowledge_bases = {}
        self.category_mappings = {}
        self.health_mappings = {}
        self.ingredient_mappings = {}
        self.synonym_mappings = {}
        
        self._load_knowledge_bases()
        self._build_search_mappings()
    
    def _load_knowledge_bases(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        knowledge_files = [
            "knowledge_base.json",
            "knowledge_base_new.json", 
            "knowledge_base_fixed.json"
        ]
        
        for kb_file in knowledge_files:
            try:
                with open(kb_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.knowledge_bases[kb_file] = data
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {kb_file} ({len(data)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤)")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {kb_file}: {e}")
    
    def _build_search_mappings(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        print("üîç –°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã
        all_products = []
        for kb_data in self.knowledge_bases.values():
            all_products.extend(kb_data)
        
        # –ò–Ω–¥–µ–∫—Å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_keywords = {
            "–∏–º–º—É–Ω–∏—Ç–µ—Ç": ["–∏–º–º—É–Ω", "–∑–∞—â–∏—Ç", "–ø—Ä–æ—Å—Ç—É–¥", "–≥—Ä–∏–ø–ø", "–≤–∏—Ä—É—Å", "–±–∞–∫—Ç–µ—Ä–∏", "–∏–Ω—Ñ–µ–∫—Ü"],
            "–ø–µ—á–µ–Ω—å": ["–ø–µ—á–µ–Ω", "–≥–µ–ø–∞—Ç", "–¥–µ—Ç–æ–∫—Å", "–æ—á–∏—â–µ–Ω", "—Ç–æ–∫—Å–∏–Ω", "—Å–∏–ª–∏—Ü–∏—Ç–∏–Ω", "—Å–∏–ª–∏–º–∞—Ä–∏–Ω", "—Ä–∞—Å—Ç–æ—Ä–æ–ø—à"],
            "—Å–µ—Ä–¥—Ü–µ": ["—Å–µ—Ä–¥—Ü", "—Å–æ—Å—É–¥", "–¥–∞–≤–ª–µ–Ω", "—Ö–æ–ª–µ—Å—Ç–µ—Ä", "–∫—Ä–æ–≤–æ–æ–±—Ä–∞—â–µ–Ω"],
            "–∫–æ–∂–∞": ["–∫–æ–∂", "–¥–µ—Ä–º–∞—Ç", "—Å—ã–ø—å", "–∞–∫–Ω–µ", "—ç–∫–∑–µ–º", "–ø—Å–æ—Ä–∏–∞–∑"],
            "–ø–∏—â–µ–≤–∞—Ä–µ–Ω–∏–µ": ["–ø–∏—â–µ–≤–∞—Ä", "–∂–µ–ª—É–¥–æ–∫", "–∫–∏—à–µ—á–Ω–∏–∫", "–≥–∞—Å—Ç—Ä–∏—Ç", "–¥–∏–∞—Ä–µ—è"],
            "–Ω–µ—Ä–≤—ã": ["–Ω–µ—Ä–≤", "—Å—Ç—Ä–µ—Å—Å", "–¥–µ–ø—Ä–µ—Å—Å", "—Ç—Ä–µ–≤–æ–≥", "–±–µ—Å—Å–æ–Ω–Ω–∏—Ü", "—É—Å–ø–æ–∫–æ–µ–Ω"],
            "–∫–æ—Å—Ç–∏": ["–∫–æ—Å—Ç", "—Å—É—Å—Ç–∞–≤", "–∞—Ä—Ç—Ä–∏—Ç", "–æ—Å—Ç–µ–æ–ø–æ—Ä–æ–∑", "–∫–∞–ª—å—Ü–∏–π"],
            "—ç–Ω–µ—Ä–≥–∏—è": ["—ç–Ω–µ—Ä–≥", "—É—Å—Ç–∞–ª–æ—Å—Ç", "–≤—è–ª–æ—Å—Ç", "—Ç–æ–Ω—É—Å", "–±–æ–¥—Ä–æ—Å—Ç"],
            "–≤–∏—Ç–∞–º–∏–Ω—ã": ["–≤–∏—Ç–∞–º–∏–Ω", "–º–∏–Ω–µ—Ä–∞–ª", "–º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç", "–∞–≤–∏—Ç–∞–º–∏–Ω–æ–∑"],
            "–¥—ã—Ö–∞–Ω–∏–µ": ["–¥—ã—Ö–∞–Ω", "–ª–µ–≥–∫", "–±—Ä–æ–Ω—Ö", "–∞—Å—Ç–º", "–∫–∞—à–µ–ª—å"]
        }
        
        for category, keywords in category_keywords.items():
            self.category_mappings[category] = []
            for product in all_products:
                product_text = self._get_full_product_text(product).lower()
                if any(keyword in product_text for keyword in keywords):
                    self.category_mappings[category].append(product)
        
        # –ò–Ω–¥–µ–∫—Å –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º –∑–¥–æ—Ä–æ–≤—å—è
        health_conditions = {
            "–±—Ä–æ–Ω—Ö–∏—Ç": ["–±—Ä–æ–Ω—Ö–∏—Ç", "–∫–∞—à–µ–ª—å", "–º–æ–∫—Ä–æ—Ç–∞", "–¥—ã—Ö–∞—Ç–µ–ª—å–Ω", "–ª–µ–≥–æ—á–Ω"],
            "–ø—Ä–æ—Å—Ç—É–¥–∞": ["–ø—Ä–æ—Å—Ç—É–¥", "–û–†–í–ò", "–Ω–∞—Å–º–æ—Ä–∫", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä", "–æ–∑–Ω–æ–±"],
            "–≥–∞—Å—Ç—Ä–∏—Ç": ["–≥–∞—Å—Ç—Ä–∏—Ç", "–∂–µ–ª—É–¥–æ–∫", "–∏–∑–∂–æ–≥", "–±–æ–ª—å –≤ –∂–µ–ª—É–¥–∫–µ"],
            "—Å—Ç—Ä–µ—Å—Å": ["—Å—Ç—Ä–µ—Å—Å", "–Ω–µ—Ä–≤–æ–∑–Ω–æ—Å—Ç", "—Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç", "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω"],
            "–±–µ—Å—Å–æ–Ω–Ω–∏—Ü–∞": ["–±–µ—Å—Å–æ–Ω–Ω–∏—Ü", "—Å–æ–Ω", "–∑–∞—Å—ã–ø–∞–Ω", "–ø—Ä–æ–±—É–∂–¥–µ–Ω"],
            "—É—Å—Ç–∞–ª–æ—Å—Ç—å": ["—É—Å—Ç–∞–ª–æ—Å—Ç", "–≤—è–ª–æ—Å—Ç", "—Å–ª–∞–±–æ—Å—Ç", "—É–ø–∞–¥–æ–∫ —Å–∏–ª"],
            "–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å": ["–≥–æ–ª–æ–≤–Ω –±–æ–ª—å", "–º–∏–≥—Ä–µ–Ω", "—Ü–µ—Ñ–∞–ª–≥–∏"],
            "–∞–ª–ª–µ—Ä–≥–∏—è": ["–∞–ª–ª–µ—Ä–≥", "–∑—É–¥", "–≤—ã—Å—ã–ø–∞–Ω", "–∫—Ä–∞–ø–∏–≤–Ω–∏—Ü"],
            "–¥–∏–∞–±–µ—Ç": ["–¥–∏–∞–±–µ—Ç", "—Å–∞—Ö–∞—Ä", "–≥–ª—é–∫–æ–∑", "–∏–Ω—Å—É–ª–∏–Ω"],
            "–≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—è": ["–≥–∏–ø–µ—Ä—Ç–æ–Ω–∏", "–¥–∞–≤–ª–µ–Ω", "–≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏"],
            "–±–æ–ª–µ–∑–Ω–∏ –ø–µ—á–µ–Ω–∏": ["–ø–µ—á–µ–Ω", "–≥–µ–ø–∞—Ç", "–∂–µ–ª—Ç", "—Ü–∏—Ä—Ä–æ–∑", "—Ñ–∏–±—Ä–æ–∑", "–∂–∏—Ä–æ–≤–∞—è –¥–∏—Å—Ç—Ä–æ—Ñ–∏—è", "—Ç–æ–∫—Å–∏–Ω", "–¥–µ—Ç–æ–∫—Å–∏–∫–∞—Ü"]
        }
        
        for condition, keywords in health_conditions.items():
            self.health_mappings[condition] = []
            for product in all_products:
                product_text = self._get_full_product_text(product).lower()
                if any(keyword in product_text for keyword in keywords):
                    self.health_mappings[condition].append(product)
        
        # –ò–Ω–¥–µ–∫—Å –ø–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º
        ingredients = {
            "—Å–µ—Ä–µ–±—Ä–æ": ["—Å–µ—Ä–µ–±—Ä", "–∞—Ä–≥–µ–Ω—Ç", "ag"],
            "–º–∞–≥–Ω–∏–π": ["–º–∞–≥–Ω–∏–π", "–º–∞–≥–Ω–∏–µ–≤", "mg"],
            "–∫–∞–ª—å—Ü–∏–π": ["–∫–∞–ª—å—Ü–∏–π", "–∫–∞–ª—å—Ü–∏–µ–≤", "ca"],
            "–≤–∏—Ç–∞–º–∏–Ω —Å": ["–≤–∏—Ç–∞–º–∏–Ω —Å", "–∞—Å–∫–æ—Ä–±–∏–Ω", "–∞—Å–∫–æ—Ä–±–∏–Ω–æ–≤–∞—è –∫–∏—Å–ª–æ—Ç–∞"],
            "–≤–∏—Ç–∞–º–∏–Ω –¥": ["–≤–∏—Ç–∞–º–∏–Ω –¥", "–≤–∏—Ç–∞–º–∏–Ω d", "—Ö–æ–ª–µ–∫–∞–ª—å—Ü–∏—Ñ–µ—Ä–æ–ª"],
            "–æ–º–µ–≥–∞-3": ["–æ–º–µ–≥–∞", "—Ä—ã–±–∏–π –∂–∏—Ä", "–∂–∏—Ä–Ω—ã–µ –∫–∏—Å–ª–æ—Ç—ã"],
            "–ø—Ä–æ–±–∏–æ—Ç–∏–∫–∏": ["–ø—Ä–æ–±–∏–æ—Ç–∏–∫", "–ª–∞–∫—Ç–æ–±–∞–∫—Ç–µ—Ä", "–±–∏—Ñ–∏–¥–æ–±–∞–∫—Ç–µ—Ä"],
            "–∞–Ω—Ç–∏–æ–∫—Å–∏–¥–∞–Ω—Ç—ã": ["–∞–Ω—Ç–∏–æ–∫—Å–∏–¥–∞–Ω—Ç", "—Ñ–ª–∞–≤–æ–Ω–æ–∏–¥", "–ø–æ–ª–∏—Ñ–µ–Ω–æ–ª"],
            "–∂–µ–Ω—å—à–µ–Ω—å": ["–∂–µ–Ω—å—à–µ–Ω—å", "ginseng"],
            "—ç—Ö–∏–Ω–∞—Ü–µ—è": ["—ç—Ö–∏–Ω–∞—Ü–µ—è", "echinacea"]
        }
        
        for ingredient, keywords in ingredients.items():
            self.ingredient_mappings[ingredient] = []
            for product in all_products:
                product_text = self._get_full_product_text(product).lower()
                if any(keyword in product_text for keyword in keywords):
                    self.ingredient_mappings[ingredient].append(product)
        
        print(f"‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –∏–Ω–¥–µ–∫—Å—ã:")
        print(f"   üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(self.category_mappings)}")
        print(f"   üè• –°–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è: {len(self.health_mappings)}")
        print(f"   üß™ –ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: {len(self.ingredient_mappings)}")
    
    def _get_full_product_text(self, product: Dict) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        text_parts = []
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        for field in ["product", "description", "composition", "dosage", "contraindications"]:
            if field in product:
                if isinstance(product[field], list):
                    text_parts.extend(product[field])
                else:
                    text_parts.append(str(product[field]))
        
        # –ü–æ–ª—å–∑–∞
        if "benefits" in product:
            if isinstance(product["benefits"], list):
                text_parts.extend(product["benefits"])
            else:
                text_parts.append(str(product["benefits"]))
        
        return " ".join(text_parts)
    
    def enhanced_search(self, query: str, max_results: int = 10, 
                       min_confidence: float = 0.3) -> List[EnhancedSearchResult]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –í–°–ï–• —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        """
        print(f"üîç –£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        normalized_query = self._normalize_query(query)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –≤—Å–µ—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        all_results = []
        strategies_used = []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        for strategy_type, strategy in self.search_strategies.items():
            print(f"   üéØ –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: {strategy.name}")
            
            strategy_results = self._apply_strategy(strategy_type, normalized_query, query)
            
            if strategy_results:
                print(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(strategy_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                strategies_used.append(strategy.name)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
                for result in strategy_results:
                    result.relevance_score *= strategy.weight
                    result.strategies_used.append(strategy.name)
                
                all_results.extend(strategy_results)
            else:
                print(f"      ‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º
        unique_results = self._deduplicate_and_rank(all_results)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        filtered_results = [r for r in unique_results if r.confidence >= min_confidence]
        
        print(f"üìä –ò—Ç–æ–≥–æ:")
        print(f"   üéØ –°—Ç—Ä–∞—Ç–µ–≥–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {len(strategies_used)}")
        print(f"   üìã –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(all_results)}")
        print(f"   ‚ú® –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(unique_results)}")
        print(f"   üé™ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_results)}")
        
        return filtered_results[:max_results]
    
    def _normalize_query(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = query.lower()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∫—Ä–æ–º–µ –≤–∞–∂–Ω–æ–π
        normalized = re.sub(r'[^\w\s\-]', ' ', normalized)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = ' '.join(normalized.split())
        
        return normalized
    
    def _apply_strategy(self, strategy_type: SearchStrategyType, 
                       normalized_query: str, original_query: str) -> List[EnhancedSearchResult]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞"""
        
        if strategy_type == SearchStrategyType.EXACT_MATCH:
            return self._exact_match_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.SEMANTIC_SEARCH:
            return self._semantic_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.SYNONYM_EXPANSION:
            return self._synonym_expansion_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.CATEGORICAL_SEARCH:
            return self._categorical_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.HEALTH_CONDITION_SEARCH:
            return self._health_condition_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.INGREDIENT_SEARCH:
            return self._ingredient_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.FUZZY_MATCH:
            return self._fuzzy_match_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.BRAND_SEARCH:
            return self._brand_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.COMPOSITIONAL_SEARCH:
            return self._compositional_search(normalized_query, original_query)
        
        elif strategy_type == SearchStrategyType.FALLBACK_BROAD:
            return self._fallback_broad_search(normalized_query, original_query)
        
        return []
    
    def _exact_match_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
        results = []
        
        for kb_name, products in self.knowledge_bases.items():
            for product in products:
                product_text = self._get_full_product_text(product).lower()
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã
                if query in product_text:
                    score = 1.0
                    
                    # –ë–æ–Ω—É—Å –µ—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
                    if query in product.get("product", "").lower():
                        score = 1.2
                    
                    results.append(EnhancedSearchResult(
                        content=product_text[:500],
                        product=product.get("product", ""),
                        chunk_type="exact_match",
                        relevance_score=score,
                        confidence=0.95,
                        source=kb_name,
                        metadata={"match_type": "exact", "query": query}
                    ))
        
        return results
    
    def _semantic_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
        if not self.vector_db:
            return []
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º vector_db –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            vector_results = self.vector_db.search(original, limit=5)
            
            results = []
            for vr in vector_results:
                results.append(EnhancedSearchResult(
                    content=vr.chunk.text,
                    product=vr.chunk.product,
                    chunk_type=vr.chunk.chunk_type,
                    relevance_score=vr.score,
                    confidence=min(vr.score, 0.9),
                    source=vr.source,
                    metadata={"search_type": "semantic"}
                ))
            
            return results
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _categorical_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        results = []
        
        for category, products in self.category_mappings.items():
            if category in query or any(keyword in query for keyword in category.split()):
                for product in products:
                    results.append(EnhancedSearchResult(
                        content=self._get_full_product_text(product)[:500],
                        product=product.get("product", ""),
                        chunk_type="category_match",
                        relevance_score=0.8,
                        confidence=0.7,
                        source="category_index",
                        metadata={"category": category, "match_type": "categorical"}
                    ))
        
        return results
    
    def _health_condition_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º –∑–¥–æ—Ä–æ–≤—å—è"""
        results = []
        
        for condition, products in self.health_mappings.items():
            condition_keywords = condition.split()
            if any(keyword in query for keyword in condition_keywords):
                for product in products:
                    results.append(EnhancedSearchResult(
                        content=self._get_full_product_text(product)[:500],
                        product=product.get("product", ""),
                        chunk_type="health_condition",
                        relevance_score=0.85,
                        confidence=0.8,
                        source="health_index",
                        metadata={"condition": condition, "match_type": "health"}
                    ))
        
        return results
    
    def _ingredient_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º"""
        results = []
        
        for ingredient, products in self.ingredient_mappings.items():
            ingredient_keywords = ingredient.split()
            if any(keyword in query for keyword in ingredient_keywords):
                for product in products:
                    results.append(EnhancedSearchResult(
                        content=self._get_full_product_text(product)[:500],
                        product=product.get("product", ""),
                        chunk_type="ingredient_match",
                        relevance_score=0.75,
                        confidence=0.75,
                        source="ingredient_index",
                        metadata={"ingredient": ingredient, "match_type": "ingredient"}
                    ))
        
        return results
    
    def _synonym_expansion_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        
        # –ü—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        synonyms_map = {
            '–º–∞–≥–Ω–∏–π': ['mg', '–º–∞–≥–Ω–∏–µ–≤—ã–π'],
            '–∫–∞–ª—å—Ü–∏–π': ['calcium', '–∫–∞–ª—å—Ü–∏–µ–≤—ã–π'],
            '–≤–∏—Ç–∞–º–∏–Ω': ['vitamin', '–≤–∏—Ç'],
            '–ø–µ—á–µ–Ω—å': ['–≥–µ–ø–∞—Ç–æ', 'liver', '–ø–µ—á–µ–Ω–æ—á–Ω—ã–π'],
            '–∏–º–º—É–Ω–∏—Ç–µ—Ç': ['immunity', '–∏–º–º—É–Ω–Ω—ã–π', '–∑–∞—â–∏—Ç–∞'],
            '—Å—Ç—Ä–µ—Å—Å': ['stress', '–Ω–µ—Ä–≤—ã', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ'],
            '—Å–æ–Ω': ['sleep', '–±–µ—Å—Å–æ–Ω–Ω–∏—Ü–∞', '—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–∏–µ']
        }
        
        try:
            expanded_terms = []
            words = original.lower().split()
            
            for word in words:
                expanded_terms.append(word)
                if word in synonyms_map:
                    expanded_terms.extend(synonyms_map[word])
            
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            expanded_query = ' '.join(expanded_terms)
            
            if expanded_query != original:
                # –ò—â–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É
                return self._exact_match_search(expanded_query.lower(), expanded_query)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏: {e}")
        
        return []
    
    def _fuzzy_match_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫"""
        results = []
        query_words = query.split()
        
        for kb_name, products in self.knowledge_bases.items():
            for product in products:
                product_text = self._get_full_product_text(product).lower()
                product_words = product_text.split()
                
                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
                max_similarity = 0
                for query_word in query_words:
                    for product_word in product_words:
                        if len(query_word) > 3 and len(product_word) > 3:
                            similarity = SequenceMatcher(None, query_word, product_word).ratio()
                            if similarity > max_similarity:
                                max_similarity = similarity
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
                if max_similarity > 0.8:
                    results.append(EnhancedSearchResult(
                        content=product_text[:500],
                        product=product.get("product", ""),
                        chunk_type="fuzzy_match",
                        relevance_score=max_similarity * 0.6,
                        confidence=max_similarity * 0.7,
                        source=kb_name,
                        metadata={"similarity": max_similarity, "match_type": "fuzzy"}
                    ))
        
        return results
    
    def _brand_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –±—Ä–µ–Ω–¥–∞–º –∏ –ª–∏–Ω–µ–π–∫–∞–º"""
        brand_keywords = [
            "–∞–≤—Ä–æ—Ä–∞", "aurora", "–±–∞—Ä—Å", "bars", "mg", "–≤–∏—Ç–∞–º–∏–Ω", "—Å–æ–ª–±–µ—Ä—Ä–∏", 
            "–∞—Ä–≥–µ–Ω—Ç", "–±–∏—Ç–µ—Ä–æ–Ω", "–≥–µ–ø–æ—Å–∏–Ω", "—Å–∏–º–±–∏–æ–Ω", "–µ–ª–æ–º–∏–ª"
        ]
        
        results = []
        for keyword in brand_keywords:
            if keyword in query:
                results.extend(self._exact_match_search(keyword, keyword))
        
        return results
    
    def _compositional_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ü–æ–∏—Å–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤"""
        results = []
        
        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤
        mentioned_ingredients = []
        for ingredient in self.ingredient_mappings.keys():
            if ingredient in query:
                mentioned_ingredients.append(ingredient)
        
        if len(mentioned_ingredients) >= 2:
            # –ò—â–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–æ–≤
            for kb_name, products in self.knowledge_bases.items():
                for product in products:
                    product_text = self._get_full_product_text(product).lower()
                    
                    contained_ingredients = 0
                    for ingredient in mentioned_ingredients:
                        if ingredient in product_text:
                            contained_ingredients += 1
                    
                    if contained_ingredients >= 2:
                        score = contained_ingredients / len(mentioned_ingredients)
                        results.append(EnhancedSearchResult(
                            content=product_text[:500],
                            product=product.get("product", ""),
                            chunk_type="compositional",
                            relevance_score=score * 0.55,
                            confidence=score * 0.6,
                            source=kb_name,
                            metadata={
                                "ingredients_found": contained_ingredients,
                                "ingredients_total": len(mentioned_ingredients),
                                "match_type": "compositional"
                            }
                        ))
        
        return results
    
    def _fallback_broad_search(self, query: str, original: str) -> List[EnhancedSearchResult]:
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–∞–¥–µ–∂–¥–∞"""
        results = []
        query_words = [word for word in query.split() if len(word) > 2]
        
        if not query_words:
            return results
        
        for kb_name, products in self.knowledge_bases.items():
            for product in products:
                product_text = self._get_full_product_text(product).lower()
                
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –ø—Ä–æ–¥—É–∫—Ç–µ
                word_matches = 0
                for word in query_words:
                    if word in product_text:
                        word_matches += 1
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if word_matches > 0:
                    score = word_matches / len(query_words)
                    results.append(EnhancedSearchResult(
                        content=product_text[:500],
                        product=product.get("product", ""),
                        chunk_type="broad_fallback",
                        relevance_score=score * 0.3,
                        confidence=score * 0.4,
                        source=kb_name,
                        metadata={
                            "word_matches": word_matches,
                            "total_words": len(query_words),
                            "match_type": "broad"
                        }
                    ))
        
        return results
    
    def _deduplicate_and_rank(self, results: List[EnhancedSearchResult]) -> List[EnhancedSearchResult]:
        """–£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞ (—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ)
        immunity_priorities = {
            # –ü–†–Ø–ú–û–ï –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞
            '–ê—Ä–≥–µ–Ω—Ç-–ú–∞–∫—Å': 100,
            '–ê—Ä–≥–µ–Ω—Ç –ú–∞–∫—Å': 100,
            '–ö–æ—à–∞—á–∏–π –ö–æ–≥–æ—Ç—å': 95,
            '–ò–Ω-–ê—É—Ä–∏–Ω': 90,
            '–ë–ê–†–°-2': 85,
            '–í–∏—Ç–∞–º–∏–Ω –°': 80,
            '–û—Ä–∞–Ω–∂ –î–µ–π': 75,  # –í–∏—Ç–∞–º–∏–Ω –° –≤ —É–¥–æ–±–Ω–æ–π —Ñ–æ—Ä–º–µ
            
            # –ö–û–°–í–ï–ù–ù–û–ï –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–º–º—É–Ω–∏—Ç–µ—Ç (–æ–±—â–µ—É–∫—Ä–µ–ø–ª—è—é—â–∏–µ)
            '–°–æ–ª–±–µ—Ä—Ä–∏-H': 60,      # –ê–Ω—Ç–∏–æ–∫—Å–∏–¥–∞–Ω—Ç, –∫–æ—Å–≤–µ–Ω–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
            '–ë–∏—Ç–µ—Ä–æ–Ω-H': 55,       # –ê–Ω—Ç–∏–æ–∫—Å–∏–¥–∞–Ω—Ç, –∫–æ—Å–≤–µ–Ω–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
            '–í–∏—Ç–∞–º–∏–Ω—ã –≥—Ä—É–ø–ø—ã –í': 45,  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–µ—Ä–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Å–≤–µ–Ω–Ω–æ
            '–û–º–µ–≥–∞-3': 40,            # –ü—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω–æ–µ, –∫–æ—Å–≤–µ–Ω–Ω–æ
            '–†—É–º–∞—Ä–∏–Ω –≠–∫—Å—Ç—Ä–∞': 30      # –†–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –æ—á–µ–Ω—å –∫–æ—Å–≤–µ–Ω–Ω–æ
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º
        product_groups = defaultdict(list)
        for result in results:
            product_groups[result.product].append(result)
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
        unique_results = []
        for product, group in product_groups.items():
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ relevance_score
            group.sort(key=lambda x: x.relevance_score, reverse=True)
            best_result = group[0]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞
            if any(keyword in result.metadata.get('category', '') or 
                   keyword in result.content.lower() 
                   for keyword in ['–∏–º–º—É–Ω', 'immunity', '–∑–∞—â–∏—Ç'] 
                   for result in group):
                priority_score = immunity_priorities.get(product, 50)
                best_result.relevance_score = max(best_result.relevance_score, priority_score / 100.0)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            all_strategies = set()
            total_confidence = 0
            for result in group:
                all_strategies.update(result.strategies_used)
                total_confidence += result.confidence
            
            best_result.strategies_used = list(all_strategies)
            best_result.confidence = min(total_confidence / len(group), 1.0)
            
            unique_results.append(best_result)
        
        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ score, –ø–æ—Ç–æ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        unique_results.sort(key=lambda x: (-x.relevance_score, -x.confidence, x.product))
        
        return unique_results

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
enhanced_search_engine = None

def initialize_enhanced_search(vector_db=None, nlp_processor=None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—Å–∏–ª–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞"""
    global enhanced_search_engine
    enhanced_search_engine = EnhancedSearchEngine(vector_db, nlp_processor)
    return enhanced_search_engine
