# enhanced_vector_db.py
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from sentence_transformers import SentenceTransformer
import json
import os
import time
import hashlib
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from text_preprocessor import text_preprocessor

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
load_dotenv()

@dataclass
class KnowledgeChunk:
    product: str
    chunk_type: str  # benefits, composition, dosage, contraindications, description
    text: str
    metadata: Dict[str, Any]

@dataclass
class SearchResult:
    chunk: KnowledgeChunk
    score: float
    source: str

class ResponseCache:
    def __init__(self, max_size: int = 100):
        self.cache = {}
        self.access_count = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[str]:
        if key in self.cache:
            self.access_count[key] = self.access_count.get(key, 0) + 1
            return self.cache[key]
        return None
    
    def set(self, key: str, value: str):
        if len(self.cache) >= self.max_size:
            # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —ç–ª–µ–º–µ–Ω—Ç
            least_used = min(self.access_count.items(), key=lambda x: x[1])
            del self.cache[least_used[0]]
            del self.access_count[least_used[0]]
        
        self.cache[key] = value
        self.access_count[key] = 1
    
    def clear(self):
        self.cache.clear()
        self.access_count.clear()

class EnhancedVectorDB:
    def __init__(self):
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Qdrant Cloud
        self.client = QdrantClient(
            url=os.getenv("QDRANT_URL"),
            api_key=os.getenv("QDRANT_API_KEY"),
            https=True
        )
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –ö—ç—à –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        self.response_cache = ResponseCache()
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (–µ—Å–ª–∏ –µ—â—ë –Ω–µ —Å–æ–∑–¥–∞–Ω–∞)
        self.collection_name = "aurora_knowledge_chunks"
        self._init_collection()
        
        # –•—ç—à –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
        # –í–ê–ñ–ù–û: —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Å—Ç–æ–π —Ö—ç—à, —á—Ç–æ–±—ã –ø–µ—Ä–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–ª–∞—Å—å
        self.last_hash = ""
        
    def _init_collection(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ Qdrant"""
        try:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=384, distance=Distance.COSINE)
            )
            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª—è chunk_type –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            try:
                from qdrant_client.models import PayloadSchemaType, CreateFieldIndex
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="chunk_type",
                    field_schema=PayloadSchemaType.KEYWORD
                )
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="product",
                    field_schema=PayloadSchemaType.KEYWORD
                )
                print("‚úÖ –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è chunk_type –∏ product —Å–æ–∑–¥–∞–Ω—ã")
            except Exception as idx_error:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å: {idx_error}")
                
        except Exception as e:
            print(f"‚ÑπÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            try:
                from qdrant_client.models import PayloadSchemaType, CreateFieldIndex
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="chunk_type", 
                    field_schema=PayloadSchemaType.KEYWORD
                )
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="product", 
                    field_schema=PayloadSchemaType.KEYWORD
                )
                print("‚úÖ –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è chunk_type –∏ product –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
            except Exception as idx_error:
                print(f"‚ÑπÔ∏è –ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: {idx_error}")
    
    def _get_file_hash(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ö—ç—à —Ñ–∞–π–ª–æ–≤ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            main_hash = hashlib.md5(open("knowledge_base.json", "rb").read()).hexdigest()
            new_hash = hashlib.md5(open("knowledge_base_new.json", "rb").read()).hexdigest()
            return hashlib.md5((main_hash + new_hash).encode()).hexdigest()
        except FileNotFoundError:
            return ""
    
    def _load_knowledge(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ –æ–±–æ–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        knowledge_data = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        try:
            with open("knowledge_base.json", "r", encoding="utf-8") as f:
                main_data = json.load(f)
                knowledge_data.extend(main_data)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(main_data)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ knowledge_base.json")
        except FileNotFoundError:
            print("‚ö†Ô∏è –§–∞–π–ª knowledge_base.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è knowledge_base.json: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        try:
            with open("knowledge_base_new.json", "r", encoding="utf-8") as f:
                new_data = json.load(f)
                knowledge_data.extend(new_data)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_data)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–∑ knowledge_base_new.json")
        except FileNotFoundError:
            print("‚ö†Ô∏è –§–∞–π–ª knowledge_base_new.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è knowledge_base_new.json: {e}")
        
        return knowledge_data
    
    def _create_chunks(self, knowledge_data: List[Dict]) -> List[KnowledgeChunk]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–º—ã—Å–ª–æ–≤—ã–µ —á–∞–Ω–∫–∏"""
        chunks = []
        
        for item in knowledge_data:
            product = item["product"]
            
            # –ß–∞–Ω–∫ —Å –ø–æ–ª–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–¥—É–∫—Ç–∞ (–¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –æ—Ç–≤–µ—Ç–æ–≤)
            if item.get("description"):
                cleaned_description = text_preprocessor.preprocess_knowledge_base_text(item["description"])
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="description",
                    text=cleaned_description,
                    metadata={
                        "category": item.get("category", ""),
                        "form": item.get("form", ""),
                        "product_id": item.get("id", ""),
                        "use_cases": item.get("benefits", [])[:3],  # –ü–µ—Ä–≤—ã–µ 3 —Å–ª—É—á–∞—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
                        "for_whom": ["–≤–∑—Ä–æ—Å–ª—ã–µ"],  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å –∫—Ä–∞—Ç–∫–∏–º –æ–ø–∏—Å–∞–Ω–∏–µ–º (–¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
            if item.get("short_description"):
                cleaned_short_description = text_preprocessor.preprocess_knowledge_base_text(item["short_description"])
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="short_description",
                    text=cleaned_short_description,
                    metadata={
                        "category": item.get("category", ""),
                        "form": item.get("form", ""),
                        "product_id": item.get("id", ""),
                        "use_cases": item.get("short_benefits", item.get("benefits", []))[:3],
                        "for_whom": ["–≤–∑—Ä–æ—Å–ª—ã–µ"],
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å –ø–æ–ª–Ω—ã–º–∏ –ø–æ–∫–∞–∑–∞–Ω–∏—è–º–∏ –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é
            if item.get("benefits"):
                benefits_text = "; ".join(item["benefits"])
                cleaned_benefits = text_preprocessor.preprocess_knowledge_base_text(f"–ü–æ–∫–∞–∑–∞–Ω–∏—è –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é: {benefits_text}")
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="benefits",
                    text=cleaned_benefits,
                    metadata={
                        "category": item.get("category", ""),
                        "use_cases": item["benefits"],
                        "product_id": item.get("id", ""),
                        "for_whom": ["–≤–∑—Ä–æ—Å–ª—ã–µ"],
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å –∫—Ä–∞—Ç–∫–∏–º–∏ –ø–æ–∫–∞–∑–∞–Ω–∏—è–º–∏ (–¥–ª—è –ø–æ–∏—Å–∫–∞)
            if item.get("short_benefits"):
                short_benefits_text = "; ".join(item["short_benefits"])
                cleaned_short_benefits = text_preprocessor.preprocess_knowledge_base_text(f"–ö—Ä–∞—Ç–∫–∏–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è: {short_benefits_text}")
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="short_benefits",
                    text=cleaned_short_benefits,
                    metadata={
                        "category": item.get("category", ""),
                        "use_cases": item["short_benefits"],
                        "product_id": item.get("id", ""),
                        "for_whom": ["–≤–∑—Ä–æ—Å–ª—ã–µ"],
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å —Å–æ—Å—Ç–∞–≤–æ–º
            if item.get("composition"):
                cleaned_composition = text_preprocessor.preprocess_knowledge_base_text(f"–°–æ—Å—Ç–∞–≤: {item['composition']}")
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="composition",
                    text=cleaned_composition,
                    metadata={
                        "category": item.get("category", ""),
                        "product_id": item.get("id", ""),
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å –¥–æ–∑–∏—Ä–æ–≤–∫–æ–π
            if item.get("dosage"):
                dosage_text = item["dosage"]
                if item.get("duration"):
                    dosage_text += f", –∫—É—Ä—Å {item['duration']}"
                cleaned_dosage = text_preprocessor.preprocess_knowledge_base_text(f"–°–ø–æ—Å–æ–± –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: {dosage_text}")
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="dosage",
                    text=cleaned_dosage,
                    metadata={
                        "category": item.get("category", ""),
                        "product_id": item.get("id", ""),
                        "source": "knowledge_base.json"
                    }
                ))
            
            # –ß–∞–Ω–∫ —Å –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è–º–∏
            if item.get("contraindications"):
                chunks.append(KnowledgeChunk(
                    product=product,
                    chunk_type="contraindications",
                    text=f"–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è: {item['contraindications']}",
                    metadata={
                        "category": item.get("category", ""),
                        "product_id": item.get("id", ""),
                        "source": "knowledge_base.json"
                    }
                ))
        
        return chunks
    
    def index_knowledge(self):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
        try:
            current_hash = self._get_file_hash()
            if current_hash == self.last_hash and current_hash != "":
                return  # –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
            
            print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
            knowledge_data = self._load_knowledge()
            
            if not knowledge_data:
                print("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
                return
            
            chunks = self._create_chunks(knowledge_data)
            points = []
            
            for i, chunk in enumerate(chunks):
                # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                full_text = f"{chunk.product}: {chunk.text}"
                vector = self.model.encode(full_text).tolist()
                
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
                point_id = f"{chunk.metadata.get('product_id', '')}_{chunk.chunk_type}_{i}"
                point_id_hash = hashlib.md5(point_id.encode()).hexdigest()
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º payload
                payload = {
                    "product": chunk.product,
                    "chunk_type": chunk.chunk_type,
                    "text": chunk.text,
                    "full_text": full_text,
                    **chunk.metadata
                }
                
                points.append(PointStruct(
                    id=point_id_hash,
                    vector=vector,
                    payload=payload
                ))
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            self.client.recreate_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=384, distance=Distance.COSINE)
            )
            
            # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            try:
                from qdrant_client.models import PayloadSchemaType
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="chunk_type",
                    field_schema=PayloadSchemaType.KEYWORD
                )
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="product",
                    field_schema=PayloadSchemaType.KEYWORD
                )
                print("‚úÖ –ò–Ω–¥–µ–∫—Å—ã —Å–æ–∑–¥–∞–Ω—ã –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
            except Exception as idx_error:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤: {idx_error}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–∫–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
            batch_size = 50
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.client.upload_points(
                    collection_name=self.collection_name,
                    points=batch
                )
            
            self.last_hash = current_hash
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –±–∞–∑—ã
            self.response_cache.clear()
            
            print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(knowledge_data)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {e}")
    
    def search(self, query: str, 
               filters: Optional[Dict[str, Any]] = None, 
               limit: int = 3) -> List[SearchResult]:
        """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
            query_vector = self.model.encode(query).tolist()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            search_filter = None
            if filters:
                conditions = []
                for key, value in filters.items():
                    if isinstance(value, list):
                        for v in value:
                            conditions.append(
                                FieldCondition(key=key, match=MatchValue(value=v))
                            )
                    else:
                        conditions.append(
                            FieldCondition(key=key, match=MatchValue(value=value))
                        )
                
                if conditions:
                    search_filter = Filter(should=conditions)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                query_filter=search_filter,
                limit=limit
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            search_results = []
            for hit in results:
                chunk = KnowledgeChunk(
                    product=hit.payload["product"],
                    chunk_type=hit.payload["chunk_type"],
                    text=hit.payload["text"],
                    metadata={k: v for k, v in hit.payload.items() 
                             if k not in ["product", "chunk_type", "text", "full_text"]}
                )
                
                search_results.append(SearchResult(
                    chunk=chunk,
                    score=hit.score,
                    source=hit.payload.get("source", "unknown")
                ))
            
            return search_results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []
    
    def search_by_product(self, product_name: str, limit: int = 5) -> List[SearchResult]:
        """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø—Ä–æ–¥—É–∫—Ç—É"""
        filters = {"product": product_name}
        return self.search("", filters=filters, limit=limit)
    
    def search_by_use_case(self, use_case: str, limit: int = 3) -> List[SearchResult]:
        """–ò—â–µ—Ç –ø—Ä–æ–¥—É–∫—Ç—ã –ø–æ —Å–ª—É—á–∞—é –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞
        normalized_query = text_preprocessor.normalize_for_search(use_case)
        
        # –ò—â–µ–º –≤ —á–∞–Ω–∫–∞—Ö benefits –∏ description
        results_benefits = self.search(normalized_query, filters={"chunk_type": "benefits"}, limit=limit)
        results_desc = self.search(normalized_query, filters={"chunk_type": "description"}, limit=limit)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        all_results = results_benefits + results_desc
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        return all_results[:limit]
    
    def get_cached_response(self, query: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        normalized_query = self._normalize_query_for_cache(query)
        cache_key = hashlib.md5(normalized_query.encode()).hexdigest()
        return self.response_cache.get(cache_key)
    
    def _normalize_query_for_cache(self, query: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = query.lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        import re
        normalized = re.sub(r'[^\w\s]', ' ', normalized)
        
        # –°–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞
        immunity_synonyms = [
            '–¥–ª—è —É–∫—Ä–µ–ø–ª–µ–Ω–∏—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞', '—É–∫—Ä–µ–ø–ª–µ–Ω–∏—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞', '–∏–º–º—É–Ω–∏—Ç–µ—Ç', 
            '–¥–ª—è –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞', '—É–∫—Ä–µ–ø–∏—Ç—å –∏–º–º—É–Ω–∏—Ç–µ—Ç', '–ø–æ–≤—ã—Å–∏—Ç—å –∏–º–º—É–Ω–∏—Ç–µ—Ç',
            '–ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å –∏–º–º—É–Ω–∏—Ç–µ—Ç', '–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–º–º—É–Ω–∏—Ç–µ—Ç–∞'
        ]
        
        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ —Å–∏–Ω–æ–Ω–∏–º—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–µ—Ä–º–∏–Ω
        for synonym in immunity_synonyms:
            if synonym in normalized:
                normalized = '–∏–º–º—É–Ω–∏—Ç–µ—Ç'
                break
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = ' '.join(normalized.split())
        
        return normalized
    
    def cache_response(self, query: str, response: str):
        """–ö—ç—à–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —á—Ç–æ –∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—ç—à–∞
        normalized_query = self._normalize_query_for_cache(query)
        cache_key = hashlib.md5(normalized_query.encode()).hexdigest()
        self.response_cache.set(cache_key, response)
    
    def start_auto_update(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã"""
        def auto_update():
            while True:
                time.sleep(10)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
                try:
                    self.index_knowledge()
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {e}")
        
        import threading
        thread = threading.Thread(target=auto_update, daemon=True)
        thread.start()
        print("üîÑ –ê–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∑–∞–ø—É—â–µ–Ω–æ")

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
enhanced_vector_db = EnhancedVectorDB()
