"""
LLM Service - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ LLM —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
"""
import os
import requests
import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
from frontend.services.prompts import get_prompt_manager, IntentType
from frontend.services.cache_service import cache_service
from frontend.utils.synonyms import expand_query_with_synonyms
from frontend.utils.special_handlers import (
    is_small_talk,
    is_immunity_query,
    enhance_context_with_special_instructions
)

load_dotenv()

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç LLM"""
    text: str
    products: List[Dict[str, Any]]
    intent: str
    confidence: float
    cached: bool = False


class LLMService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —Å –ø–æ–ª–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å–µ—Ä–≤–∏—Å–∞"""
        self.api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º API URL
        if os.getenv("OPENROUTER_API_KEY"):
            self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        else:
            self.api_url = "https://api.openai.com/v1/chat/completions"
        
        # –ú–æ–¥–µ–ª—å
        model_name = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        if "GPT-5" in model_name or "gpt-5" in model_name:
            model_name = "openai/gpt-4o-mini"
        elif "Chat" in model_name:
            model_name = "openai/gpt-3.5-turbo"
        self.model = model_name
        
        # –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
        self.prompt_manager = get_prompt_manager()
        
        logger.info(f"LLM Service initialized with model: {self.model}")
        logger.info("Integrated: PromptManager, CacheService, Synonyms, SpecialHandlers")
    
    async def process_query(
        self,
        user_query: str,
        context: Optional[str] = None,
        products: Optional[List[Dict]] = None,
        intent: Optional[IntentType] = None
    ) -> LLMResponse:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ LLM
        
        Args:
            user_query: –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            products: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
            intent: —Ç–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            LLMResponse —Å –æ—Ç–≤–µ—Ç–æ–º
        """
        logger.info(f"Processing query: {user_query[:50]}...")
        
        try:
            # ======================================
            # 1. SMALL-TALK –û–ë–†–ê–ë–û–¢–ö–ê
            # ======================================
            smalltalk_response = is_small_talk(user_query)
            if smalltalk_response:
                logger.info("Detected small-talk, returning quick response")
                return LLMResponse(
                    text=smalltalk_response,
                    products=[],
                    intent="small_talk",
                    confidence=1.0
                )
            
            # ======================================
            # 2. –ü–†–û–í–ï–†–ö–ê –ö–≠–®–ê
            # ======================================
            cached_response = cache_service.get(user_query)
            if cached_response:
                logger.info("Cache hit! Returning cached response")
                return LLMResponse(
                    text=f"{cached_response}\n\nüí° _–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∫—ç—à–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞_",
                    products=products or [],
                    intent="cached",
                    confidence=0.9,
                    cached=True
                )
            
            # ======================================
            # 3. –†–ê–°–®–ò–†–ï–ù–ò–ï –ó–ê–ü–†–û–°–ê –°–ò–ù–û–ù–ò–ú–ê–ú–ò
            # ======================================
            expanded_query = expand_query_with_synonyms(user_query)
            if expanded_query != user_query:
                logger.debug(f"Query expanded with synonyms: {len(expanded_query)} chars")
            
            # ======================================
            # 4. –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ö–û–ù–¢–ï–ö–°–¢–ê
            # ======================================
            full_context = self._build_context(context, products)
            
            # –î–æ–ø–æ–ª–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
            enhanced_context = enhance_context_with_special_instructions(
                full_context,
                user_query
            )
            
            # ======================================
            # 5. –í–´–ë–û–† –°–ò–°–¢–ï–ú–ù–û–ì–û –ü–†–û–ú–ü–¢–ê
            # ======================================
            if intent:
                system_prompt = self.prompt_manager.get_prompt(intent)
                logger.debug(f"Using prompt for intent: {intent}")
            else:
                system_prompt = self.prompt_manager.get_prompt_by_keywords(user_query)
                logger.debug("Auto-detected prompt from keywords")
            
            # ======================================
            # 6. –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –°–û–û–ë–©–ï–ù–ò–ô
            # ======================================
            messages = [
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n{enhanced_context}\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}"
                }
            ]
            
            # ======================================
            # 7. –í–´–ó–û–í LLM
            # ======================================
            response_text = await self._call_llm(messages)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å—å
            response_text += "\n\n*üìö –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞ Aurora*"
            
            # ======================================
            # 8. –ö–≠–®–ò–†–û–í–ê–ù–ò–ï –û–¢–í–ï–¢–ê
            # ======================================
            # –ö—ç—à–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            if len(user_query.strip()) > 20:
                cache_service.set(user_query, response_text)
                logger.debug("Response cached")
            
            # ======================================
            # 9. –í–û–ó–í–†–ê–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–ê
            # ======================================
            return LLMResponse(
                text=response_text,
                products=products or [],
                intent=intent.value if intent else "auto_detected",
                confidence=0.85
            )
            
        except Exception as e:
            logger.error(f"Error processing query: {e}", exc_info=True)
            return LLMResponse(
                text="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
                products=[],
                intent="error",
                confidence=0.0
            )
    
    def _build_context(
        self,
        context: Optional[str],
        products: Optional[List[Dict]]
    ) -> str:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM"""
        if not context and not products:
            return "NO_INFORMATION_FOUND"
        
        parts = []
        
        if context:
            parts.append(context)
        
        if products:
            parts.append("\n\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã:\n")
            for i, product in enumerate(products[:8], 1):  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 8
                name = product.get('product', product.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç'))
                price = product.get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')
                category = product.get('category', '')
                description = product.get('description', product.get('short_description', ''))[:200]
                
                parts.append(f"\n{i}. –ü—Ä–æ–¥—É–∫—Ç: {name}")
                if category:
                    parts.append(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
                if price:
                    parts.append(f"   –¶–µ–Ω–∞: {price}")
                if description:
                    parts.append(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {description}...")
        
        result = "\n".join(parts)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if result == "NO_INFORMATION_FOUND":
            return result
        
        return result
    
    async def _call_llm(self, messages: List[Dict]) -> str:
        """–í—ã–∑–æ–≤ LLM API"""
        if not self.api_key:
            logger.warning("No API key configured, returning fallback response")
            return self._fallback_response()
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 500
            }
            
            response = requests.post(
                self.api_url,
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                logger.error(f"LLM API error: {response.status_code} - {response.text[:200]}")
                return self._fallback_response()
                
        except Exception as e:
            logger.error(f"Error calling LLM: {e}", exc_info=True)
            return self._fallback_response()
    
    def _fallback_response(self) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        return (
            "–Ø –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –≤–∞—Å. "
            "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤—ã—à–µ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π."
        )
    
    def get_cache_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        return cache_service.get_stats()
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        cache_service.clear()
        logger.info("Cache cleared manually")


# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
llm_service = LLMService()